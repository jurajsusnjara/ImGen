BILJEŠKE:


> ZAŠTO [0,1] faila dok [-1,1] radi super ?



> opisat MNIST dataset, kolko ima slika i tako
> trenirat mnist s gan-om i dcgan-om. unutar svakog vidit koji parametri najbolje odgovaraju (slojevi, relu/tanh/sigmoid, dropout, z_dim...). onda međusobno usporedit najbolje od gan-a i najbolje od dcgan-a.
> zašto ADAM optimizer ?
> zaključak da je dcgan bolji i koristit će se za druge slike
> kad ubacim neke kompleksnije slike rezultati mogu bit dosta loši, radi toga treba posegnut za nekim naprednijim tehnikama treniranja dcgan-ova ko šta je opisano u onom norveškom radu. npr dodavat šum u ulazne slike itd.

TESTIRANJE/EVALUACIJA (pitat mentora kolko i kako testirat !!!)
> splitat dataset na train i test pa onda u one grafove s greškama uvalit i test i train vrijednosti ?
> IDEJA za evaluaciju konačno istrenirane mreže: istrenirat mrežu koja dobro već zna razlikovat stvarne od lažnih slika ili skinut neku s interneta ili koristit mrežu koja dobro zna klasificirat pa vidit jel može klasificirat i rezultate dcgan-a. kakva je razlika ovog pristupa od onog kad se ove dvi uzastopno nadmeću ? jel mogu napravit konačnu evaluaciju na temelju spremanja d_lossova i g_lossova kroz treniranje kao što sada radim ? pogledat detaljnije norveški rad ?
> GAN-ove je na kraju najbolje ocjenit tako da ljudi pogledaju slike i vide imaju li smisla
> za MNIST bi mogao uzet mrežu koja jako dobro klasificira brojeve i onda vidjet može li isto tako dobro klasificirat i umjetno generirane brojeve
> SGAM algoritam (72. stranica norveškog rada) - uzastopno se treniraju i evaluiraju dvi mreže

> spremit DCGAN mrežu nakon što istreniram, prvo za MNIST

> objasnit dekonvoluciju (transposed convolution), čitav proces, batch normalization...
	https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0
> zašto normalizacija: 
	https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c
	https://gab41.lab41.org/batch-normalization-what-the-hey-d480039a9e3b
> što je dropout i zašto ga koristit
